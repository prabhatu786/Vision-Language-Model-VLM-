{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC_KiG6Z7uz8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pandas requests Pillow tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, clear_output\n",
        "from subprocess import getoutput\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'K80' in s:gpu = 'K80'\n",
        "elif 'T4' in s:gpu = 'T4'\n",
        "elif 'P100' in s:gpu = 'P100'\n",
        "else:\n",
        "    gpu='DONT PROCEED'\n",
        "display(HTML(f\"<h1>{gpu}</h1>\"))"
      ],
      "metadata": {
        "id": "07IMJFeb7xsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Library"
      ],
      "metadata": {
        "id": "VGxRkyR4Rb95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "import os\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "F2JuAsJd7xvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configuration Parameters**"
      ],
      "metadata": {
        "id": "wBtkgcD28BGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CSV_FILE_PATH = '/content/Final_dataset.csv'\n",
        "BATCH_SIZE = 100\n",
        "MAX_WORKERS = 20\n",
        "CACHE_DIR = 'image_cache'\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/product_category.csv')\n",
        "df = pd.DataFrame(data)\n",
        "unique_categories = df['Product_category'].unique()\n",
        "unique_categories_list = unique_categories.tolist()\n"
      ],
      "metadata": {
        "id": "GrU2pVD47xye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Step 1: Data Management**"
      ],
      "metadata": {
        "id": "6svegEqZ8JTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    data = pd.read_csv(CSV_FILE_PATH)\n",
        "    df = pd.DataFrame(data)\n",
        "    required_columns = ['PRODUCT_MAIN_IMAGE_URL']\n",
        "    for col in required_columns:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"The CSV file must contain a column named '{col}'.\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(df)} image URLs from the CSV file.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"CSV file not found at path: {CSV_FILE_PATH}\")\n",
        "    exit(1)\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(f\"CSV file at {CSV_FILE_PATH} is empty.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV file: {e}\")\n",
        "    exit(1)\n"
      ],
      "metadata": {
        "id": "BDKT6A7k7x1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Step 2: Image Loading**"
      ],
      "metadata": {
        "id": "yVssyTuq8Ufx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_image_from_url(url, size=(224, 224), retries=3, backoff=5, cache_dir='image_cache'):\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "    filename = os.path.join(cache_dir, os.path.basename(url).split(\"?\")[0])\n",
        "\n",
        "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "        filename += '.jpg'\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        try:\n",
        "            img = Image.open(filename).convert(\"RGB\")\n",
        "            img = img.resize(size)\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cached image {filename}: {e}\")\n",
        "            os.remove(filename)\n",
        "\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "            img = img.resize(size)\n",
        "            img.save(filename)\n",
        "            return img\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed for {url}: {e}\")\n",
        "            time.sleep(backoff * (2 ** attempt))\n",
        "\n",
        "    print(f\"All attempts failed for {url}.\")\n",
        "    return None\n",
        "\n",
        "def download_images(urls, max_workers=20):\n",
        "    \"\"\"\n",
        "    Downloads images in parallel using ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    images = [None] * len(urls)\n",
        "\n",
        "    def fetch_image(idx, url):\n",
        "        images[idx] = load_image_from_url(url, size=(224, 224), cache_dir=CACHE_DIR)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(fetch_image, idx, url) for idx, url in enumerate(urls)]\n",
        "        for future in as_completed(futures):\n",
        "            pass\n",
        "\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "ym4Rg7gc7x5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step 3: Batch Processing*"
      ],
      "metadata": {
        "id": "Riu3471r8gzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CLIP model or processor: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "def predict_categories_batch(images, categories, model, processor, device):\n",
        "    if not images:\n",
        "        return []\n",
        "\n",
        "\n",
        "    try:\n",
        "        inputs = processor(text=categories, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during processing inputs: {e}\")\n",
        "        return [\"Error\"] * len(images)\n",
        "\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model inference: {e}\")\n",
        "        return [\"Error\"] * len(images)\n",
        "\n",
        "\n",
        "    try:\n",
        "        probs = logits_per_image.softmax(dim=1).cpu().numpy()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during probability calculation: {e}\")\n",
        "        return [\"Error\"] * len(images)\n",
        "\n",
        "\n",
        "    try:\n",
        "        predicted_indices = probs.argmax(axis=1)\n",
        "        predicted_categories = [categories[idx] for idx in predicted_indices]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during category assignment: {e}\")\n",
        "        predicted_categories = [\"Error\"] * len(images)\n",
        "\n",
        "    return predicted_categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OisZuW6mt0RT",
        "outputId": "2e708cde-208f-4a64-e723-5a77bf88c9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Step 4: Predict Categories for All Images**"
      ],
      "metadata": {
        "id": "8vcRB2JI8mqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 35\n",
        "predicted_categories = []\n",
        "num_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "\n",
        "for batch_num in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
        "    start_idx = batch_num * BATCH_SIZE\n",
        "    end_idx = min(start_idx + BATCH_SIZE, len(df))\n",
        "    batch_urls = df['PRODUCT_MAIN_IMAGE_URL'].iloc[start_idx:end_idx].tolist()\n",
        "\n",
        "\n",
        "\n",
        "    images = download_images(batch_urls, max_workers=MAX_WORKERS)\n",
        "    valid_images = []\n",
        "    valid_indices = []\n",
        "\n",
        "\n",
        "    for idx, img in enumerate(images):\n",
        "        if img is not None:\n",
        "            valid_images.append(img)\n",
        "            valid_indices.append(idx)\n",
        "        else:\n",
        "            predicted_categories.append(\"Error\")\n",
        "\n",
        "    if valid_images:\n",
        "        batch_predictions = predict_categories_batch(valid_images, unique_categories_list, model, processor, device)\n",
        "        for pred in batch_predictions:\n",
        "            predicted_categories.append(pred)\n",
        "\n",
        "    while len(predicted_categories) < end_idx:\n",
        "        predicted_categories.append(\"Error\")\n",
        "\n",
        "df['predicted_category'] = predicted_categories[:len(df)]"
      ],
      "metadata": {
        "id": "123SdWBP7x_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Result Storage**"
      ],
      "metadata": {
        "id": "3i0f6k3Q8wF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_csv_path = \"product_categories_predictions.csv\"\n",
        "try:\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Predictions saved to {output_csv_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving predictions to CSV: {e}\")\n",
        "\n",
        "print(df.head(150))\n"
      ],
      "metadata": {
        "id": "dYt9Svcf7yDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDi-U5UM7yGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4z7fsGW7yJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmLQyiKs7yP5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}